import os
import pandas as pd
from googleapiclient.discovery import build
import time

# ========== CONFIG ==========
API_KEY = 'YOUR_API_KEY'  # ðŸ›‘ Replace with your actual API key
YOUTUBE = build('youtube', 'v3', developerKey=API_KEY)

SEARCH_TERMS = ['prefect', 'airbyte', 'postgresql', 'dbt', 'hex']
PUBLISHED_AFTER = '2023-01-01T00:00:00Z'
MAX_RESULTS = 50  # Max results per page
MAX_PAGES = 2  # Limit search to 2 pages per term

# ========== HELPER FUNCTIONS ==========

def search_videos(query, seen_video_ids):
    """
    Search YouTube for videos matching the query and return new video items.
    Limits to MAX_PAGES pages per term.
    """
    video_items = []
    next_page_token = None
    page_count = 0

    while page_count < MAX_PAGES:
        request = YOUTUBE.search().list(
            q=query,
            part='id,snippet',
            type='video',
            publishedAfter=PUBLISHED_AFTER,
            maxResults=MAX_RESULTS,
            pageToken=next_page_token,
            relevanceLanguage='en',
            regionCode='NG',
            order='viewCount'
        )
        response = request.execute()
        page_items = response.get('items', [])

        # Deduplicate using seen_video_ids
        new_items = [
            item for item in page_items
            if item['id']['videoId'] not in seen_video_ids
        ]

        video_items.extend(new_items)
        for item in new_items:
            seen_video_ids.add(item['id']['videoId'])

        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break

        page_count += 1
        time.sleep(1)

    return video_items


def get_video_details(video_ids):
    """
    Fetch detailed info for a list of video IDs.
    """
    details = []
    for i in range(0, len(video_ids), 50):
        request = YOUTUBE.videos().list(
            part='snippet,statistics,contentDetails,status',
            id=','.join(video_ids[i:i + 50])
        )
        response = request.execute()
        details.extend(response.get('items', []))
        time.sleep(1)
    return details


def get_channel_details(channel_ids, seen_channel_ids):
    """
    Fetch channel details for a list of channel IDs, avoiding duplicates.
    """
    new_channel_ids = [cid for cid in channel_ids if cid not in seen_channel_ids]
    channel_info = []

    for i in range(0, len(new_channel_ids), 50):
        request = YOUTUBE.channels().list(
            part='snippet,statistics',
            id=','.join(new_channel_ids[i:i + 50])
        )
        response = request.execute()
        items = response.get('items', [])
        channel_info.extend(items)

        for item in items:
            seen_channel_ids.add(item['id'])

        time.sleep(1)
    return channel_info


def build_dataframe(videos, channels):
    """
    Merge video and channel metadata into a single dataframe.
    """
    # Build channel lookup dictionary
    channel_map = {
        ch['id']: {
            'channel_title': ch['snippet']['title'],
            'channel_description': ch['snippet'].get('description', ''),
            'channel_country': ch['snippet'].get('country', 'Unknown'),
            'channel_created_at': ch['snippet']['publishedAt'],
            'subscriber_count': ch['statistics'].get('subscriberCount', 0),
            'channel_video_count': ch['statistics'].get('videoCount', 0),
            'channel_view_count': ch['statistics'].get('viewCount', 0),
        } for ch in channels
    }

    # Compile full data rows
    data = []
    for video in videos:
        snip = video['snippet']
        stats = video.get('statistics', {})
        content = video.get('contentDetails', {})
        status = video.get('status', {})
        channel_id = snip['channelId']
        channel_data = channel_map.get(channel_id, {})

        data.append({
            'video_id': video['id'],
            'video_title': snip.get('title', ''),
            'video_description': snip.get('description', ''),
            'tags': snip.get('tags', []),
            'published_at': snip.get('publishedAt', ''),
            'category_id': snip.get('categoryId', ''),
            'live_broadcast_content': snip.get('liveBroadcastContent', ''),
            'default_language': snip.get('defaultLanguage', ''),
            'duration': content.get('duration', ''),
            'definition': content.get('definition', ''),
            'caption': content.get('caption', ''),
            'projection': content.get('projection', ''),
            'view_count': stats.get('viewCount', 0),
            'like_count': stats.get('likeCount', 0),
            'comment_count': stats.get('commentCount', 0),
            'favorite_count': stats.get('favoriteCount', 0),
            'privacy_status': status.get('privacyStatus', ''),
            'license': status.get('license', ''),
            'channel_id': channel_id,
            **channel_data
        })

    df = pd.DataFrame(data)

    # Filter to only Nigerian channels
    df = df[df['channel_country'].str.upper() == 'NG']

    return df

# ========== MAIN FUNCTION ==========

def main():
    print("ðŸš€ Starting YouTube scrape...")

    seen_video_ids = set()
    seen_channel_ids = set()
    all_search_results = []

    # Search and collect videos for each term
    for term in SEARCH_TERMS:
        print(f"ðŸ” Searching for: {term}")
        results = search_videos(term, seen_video_ids)
        all_search_results.extend(results)

    if not all_search_results:
        print("âš ï¸ No videos found.")
        return

    print(f"ðŸŽ¯ Unique videos collected: {len(seen_video_ids)}")
    video_ids = list(seen_video_ids)
    detailed_videos = get_video_details(video_ids)

    # Collect unique channel IDs
    channel_ids = list({v['snippet']['channelId'] for v in detailed_videos})
    print(f"ðŸ“º Fetching channel data for {len(channel_ids)} channels")
    channel_details = get_channel_details(channel_ids, seen_channel_ids)

    df = build_dataframe(detailed_videos, channel_details)

    if df.empty:
        print("âš ï¸ No Nigerian data found.")
        return

    df.drop_duplicates(subset='video_id', inplace=True)
    df.sort_values(by='view_count', ascending=False, inplace=True)
    df.to_csv('engineering_youtube_NG_data.csv', index=False)
    print("ðŸ’¾ Saved to engineering_youtube_NG_data.csv")

    print("âœ… Sample top videos:")
    print(df[['video_title', 'channel_title', 'view_count']].head(3))


if __name__ == '__main__':
    main()
